---
title: 'Class 9: Machine Learning pt1.'
author: "Chae Kook"
date: "2/5/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## K-means clustering

Let's try the `kmeans()` function in R to cluster some made-up example data. 

```{r}
tmp <- c(rnorm(30,-3), rnorm(30,3)) 
x <- cbind(x=tmp, y=rev(tmp))


plot(x)

```
Use the kmeans() function setting k to 2 and nstart=20
```{r}
km <- kmeans(x, centers = 2, nstart = 20)
```

Inspect/print the results
```{r}
km 
```

What is in the output object `km`I can use the `attributes()`
function to find this info 

```{r}
attributes(km)

```

Q. How many points are in each cluster?
```{r}
km$size
```

Q. What ‘component’ of your result object details
- cluster size?
- cluster assignment/membership? 
- cluster center?

```{r}
km$cluster

```

let's check how many 2s and 1s are in this vector with the `table()` function. 

```{r}
table(km$cluster)
```

Plot x colored by the kmeans cluster assignment and
      add cluster centers as blue points
```{r}
plot(x, col=km$cluster+1)
points(km$centers, col="blue", pch=15, cex=3)
```




## Hierarchical clustering in R 

The `hclust()` function is the main Hierarchical clustering method in R and it must be passed a distance matrix as input not your raw data! 

```{r}
hc <- hclust( dist(x)) 
hc
```

```{r}
plot(hc)
abline(h=6, col="red", lty=2)

```
```{r}
table(cutree(hc, h=3.5))
```

You can also ask `cutree()` for the `k` number of groups that you want.

```{r}
cutree(hc, k=5)
```

```{r}
x.grps1 <- x[grps==1]
```


### Some more messy data to cluster 

```{r}
# Step 1. Generate some example data for clustering x <- rbind(
matrix(rnorm(100, mean=0, sd=0.3), ncol = 2) 
matrix(rnorm(100, mean=1, sd=0.3), ncol = 2)
matrix(c(rnorm(50, mean=1, sd=0.3), rnorm(50, mean=0, sd=0.3)), ncol = 2)
colnames(x) <- c("x", "y")
# Step 2. Plot the data without clustering
plot(x)
# c3
 # Step 3. Generate colors for known clusters
# (just so we can compare to hclust results) 
col <- as.factor( rep(c("c1","c2","c3"), each=50) )
plot(x, col=col)
```

Q. Use the dist(), hclust(), plot() and cutree() functions to return 2 and 3 clusters
```{r}
hc <- hclust( dist(x) )
plot(hc)
```

```{r}
grps3 <- cutree(hc, k=3)
grps3
```

```{r}
table(grps3)
```

Q. How does this compare to your known 'col' groups?
```{r}
plot(x, col=grps3)
```







# Principal Component Analysis (PCA)

The main function in base R for PCA is called `prcomp()`. Here we will use PCA to examine the funny food that folks eat in the UK and N. Ireland. 

Import the CSV file first:

```{r}
x <- read.csv("UK_foods.csv", row.names=1)
x
```

Make some conventional plots 
```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```
```{r}
pairs(x, col=rainbow(10), pch=16)
```

#PCA to the rescue!

```{r}
pca <- prcomp(t(x))
```

```{r}
summary(pca)
```

```{r}
attributes(pca)
```


```{r}
pca
```


```{r}
plot (pca$x[,1],pca$x[,2], xlab="PC1 (67.4%)", ylab="PC2 (29%)")
text(pca$x[,1],pca$x[,2], labels=colnames(x), col=c("black","red","blue","darkgreen"))
```

```{r}
attributes(pca)
```












